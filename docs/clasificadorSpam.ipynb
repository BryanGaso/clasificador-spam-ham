{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador de mensajes SPAM\n",
    "## Técnica: <span style=\"color:blue\">*Bayes Ingenuo*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Sobre la técnica  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un clasificador Bayesiano ingenuo es un clasificador probabilístico fundamentado en el teorema de Bayes y una pequeña simplificación: asumimos que todas las variables involucradas en la clasificación son *independientes entre sí*; es decir, que la ocurrencia de un evento no afecta la probabilidad de ocurrencia de otro. De esta simplificación proviene el nombre de **Ingenuo**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Sobre el problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se plantea construir un clasificador de emails que nos diga con alta confianza si un email recibido es spam o ham (no spam). Para construir nuestro clasificador de spam-ham, haremos uso de un <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/\">dataset</a> que contiene un total de 5574 mensajes (al momento de la investigación) y su respectiva etiqueta: **spam** o **ham**.  \n",
    "Se puede obtener más información acerca del **dataset** <a href=\"https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\">aquí</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendremos algunas consideraciones acerca del dataset:\n",
    "- Las letras serán convertidas a minúsculas.\n",
    "- Todo número incluído en un mensaje será reemplazado por la palabra **number**.\n",
    "- Las URLs serán reemplazadas por la palabra **httpaddr**.\n",
    "- Consideraremos palabras que comúnmente se usan en los mensajes para evitar tomarlas en cuenta al momento de crear el diccionario. Estas fueron obtenidas desde este <a href=\"https://www.ranks.nl/stopwords\">enlace</a>.\n",
    "- Los símbolos como **, . ; @ # ? ! & \\$ : _** serán retirados de los mensajes para poder trabajar mayoritariamente con las palabras.\n",
    "\n",
    "> **Nota**: Si se desea usar el clasificador para un mensaje fuera de los mensajes de prueba (obtenidos también del dataset), tener en cuenta estas consideraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas necesarias necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos las siguientes:\n",
    "- <a href=\"https://www.numpy.org/devdocs/reference/\">**Numpy**</a> - manejo de arreglos y operaciones matemáticas como el logaritmo. (`pip install numpy`)\n",
    "- <a href=\"https://pandas.pydata.org/pandas-docs/stable/\">**Pandas**</a> - manejo de DataFrames para mejor control de datos. (`pip install pandas`)\n",
    "- <a href=\"https://scikit-learn.org/stable/\">**Scikt Learn**</a> -  para extracción de caraterísticas y selección de data de entrenamiento y prueba. (`pip install scikit-learn`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención del diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la creación del diccionario, se contará la cantidad de palabras presentes en los mensajes utilizados **para el entrenamiento**. Para ello, leeremos el dataset, realizaremos las conversiones necesarias (las mencionadas en **Preparando el dataset**) y dividiremos el dataset en 2 (3/4 para entrenamiento y 1/4 para prueba)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import heapq\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaramos la lista de palabras a evitar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \n",
    "    \"again\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",    \n",
    "    \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"as\", \"at\", \"be\", \"became\", \"because\", \"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"behind\", \"being\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\",\"can\", \"cannot\", \"cant\", \"could\", \"couldnt\", \"de\", \"describe\", \"do\", \"done\", \"each\", \"eg\", \"either\", \"else\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"find\",\"for\",\"found\", \"four\", \"from\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"i\", \"ie\", \"if\", \"in\", \"indeed\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\",\"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\",\"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"she\", \"should\",\"since\", \"sincere\",\"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"take\",\"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\",\n",
    "    \"this\", \"those\", \"though\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\",\n",
    "    \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \n",
    "    \"who\", \"whoever\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura del dataset. Lo almacenaremos en un objeto `DataFrame` de la biblioteca __pandas__ de __Python__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/Bryan/Documents/DjangoProjects/mysite/clasificadorSpam/SMSSpamCollection', \n",
    "                   sep='\\\\t', \n",
    "                   names=['etiqueta','mensaje'],engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cambiamos todas las letras a minúsculas*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cambiamos los enlaces por la palabra __httpaddr__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mensaje'] = df['mensaje'].str.replace('http\\S+|www.\\S+', 'httpaddr', case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cambiamos los números por la palabra __number__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mensaje'] = df['mensaje'].replace('\\d+', ' number ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Eliminamos los símbolos mencionados*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mensaje'] = df['mensaje'].replace('[,.;@#?!&$:_]+', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos mensajes para el entrenamiento y los que usaremos para la prueba.  \n",
    "La función `train_test_split` dividirá las columnas `mensaje` y `etiqueta` de nuestro `DataFrame` en dos, obteniendo así un conjunto de entrenamiento y otro para la prueba.\n",
    "> **Nota:** Trabajaremos con el conjunto de entrenamiento hasta que empecemos con la fase de prueba del sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "msj_entrenar, msj_test, etiqueta_entrenar, etiqueta_test = train_test_split(df['mensaje'], df['etiqueta'], random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un objeto `CountVectorizer` de la biblioteca __sklearn__ que nos ayudará con el conteo de palabras dentro de los mensajes.  \n",
    "La documentación del objeto la tenemos en este <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">link</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos el conteo de características (palabras por cada mensaje) y lo almacenamos en una matriz X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = cont.fit_transform(msj_entrenar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos un `DataFrame` a partir de las variables anteriores: `cont.get_feature_names` nos dará la lista de palabras que fueron encontradas en **todos** los mensajes y `X.toarray` será la matriz de ocurrencias de las palabras en los mensajes.\n",
    "> Nota: Las filas del DataFrame **no necesariamente coinciden** con los elementos de la lista de mensajes `msj_entrenar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras = cont.get_feature_names()\n",
    "matriz_frecuen=pd.DataFrame(data=X.toarray(), columns=palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos la sumatoria de las columnas del DataFrame anterior y obtenemos **EL TOTAL DE OCURRENCIAS DE CADA PALABRA**. Luego creamos un `DataFrame` para tener una mejor visualización de los datos\n",
    "> **Nota:** `np.ravel(arr)` devuelve un arreglo significativo. En este caso, la función utilizada `arr.itemset` crea el arreglo de esta forma: `[[item1, item2, item3, ...]]`; con `np.ravel` obtendremos: `[item1, item2, item3, ...]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=np.ndarray(dtype=np.int32, shape=(1, len(matriz_frecuen.keys())))\n",
    "for i, palabra in enumerate(matriz_frecuen):\n",
    "    arr.itemset(i, sum(matriz_frecuen[palabra]))\n",
    "arr = np.ravel(arr)\n",
    "\n",
    "data=pd.DataFrame({'palabra' : palabras , 'ocurrencia':arr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retiramos las palabras que se encuentran en nuestro arreglo `STOP_WORDS` y obtenemos la lista de palabras ordenadas por ocurrencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['palabra'].isin(STOP_WORDS)]\n",
    "data_ordenada=data.sort_values(by='ocurrencia', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos con cuántas palabras contamos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6813"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esas palabras, decidimos usar solo 3000 para crear nuestro **diccionario**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario=data_ordenada.head(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palabra</th>\n",
       "      <th>ocurrencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4130</th>\n",
       "      <td>number</td>\n",
       "      <td>3470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>call</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6461</th>\n",
       "      <td>ur</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117</th>\n",
       "      <td>just</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>free</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>ok</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>ltgt</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3208</th>\n",
       "      <td>know</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>day</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3411</th>\n",
       "      <td>ll</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>got</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>good</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>like</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>come</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>time</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5275</th>\n",
       "      <td>send</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3482</th>\n",
       "      <td>love</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6626</th>\n",
       "      <td>want</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>text</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>going</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3458</th>\n",
       "      <td>lor</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6207</th>\n",
       "      <td>today</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5595</th>\n",
       "      <td>sorry</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6364</th>\n",
       "      <td>txt</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5768</th>\n",
       "      <td>stop</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712</th>\n",
       "      <td>home</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>need</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>don</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>back</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>dont</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>academic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>bp</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>customers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>galileo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>brah</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>frog</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>brain</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>hockey</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>fromm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>front</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>curry</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>britney</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>british</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>challenge</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>count</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>art</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622</th>\n",
       "      <td>held</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>bulbs</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>cornwall</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>boat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>geeeee</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>aeroplane</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>dealing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>freedom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>aeronautics</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>bloodsend</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>fails</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>bloodblood</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>advise</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>agree</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          palabra  ocurrencia\n",
       "4130       number        3470\n",
       "842          call         451\n",
       "6461           ur         290\n",
       "3117         just         276\n",
       "2213         free         223\n",
       "4193           ok         204\n",
       "3509         ltgt         193\n",
       "3208         know         188\n",
       "1409          day         188\n",
       "3411           ll         183\n",
       "2429          got         182\n",
       "2410         good         182\n",
       "3362         like         180\n",
       "1137         come         175\n",
       "6171         time         162\n",
       "5275         send         151\n",
       "3482         love         151\n",
       "6626         want         147\n",
       "6051         text         146\n",
       "2398        going         136\n",
       "3458          lor         126\n",
       "6207        today         125\n",
       "5595        sorry         124\n",
       "6364          txt         124\n",
       "5768         stop         122\n",
       "2712         home         121\n",
       "3990         need         115\n",
       "1637          don         114\n",
       "455          back         111\n",
       "1641         dont         110\n",
       "...           ...         ...\n",
       "26       academic           2\n",
       "721            bp           2\n",
       "1356    customers           2\n",
       "2288      galileo           2\n",
       "723          brah           2\n",
       "2245         frog           2\n",
       "724         brain           2\n",
       "2698       hockey           2\n",
       "2247        fromm           2\n",
       "2248        front           2\n",
       "1350        curry           2\n",
       "754       britney           2\n",
       "753       british           2\n",
       "960     challenge           2\n",
       "1269        count           2\n",
       "344           art           2\n",
       "2622         held           2\n",
       "791         bulbs           2\n",
       "1246     cornwall           2\n",
       "673          boat           2\n",
       "2323       geeeee           2\n",
       "100     aeroplane           2\n",
       "1426      dealing           2\n",
       "2214      freedom           2\n",
       "99    aeronautics           2\n",
       "657     bloodsend           2\n",
       "1979        fails           2\n",
       "656    bloodblood           2\n",
       "96         advise           2\n",
       "129         agree           2\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicio del entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos un dataset con la data de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrenamiento=pd.DataFrame({'mensaje' : msj_entrenar , 'etiqueta':etiqueta_entrenar})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4180"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entrenamiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los mensajes dependiendo de su etiqueta: **spam** o **ham**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam=entrenamiento[entrenamiento['etiqueta'] =='spam']['mensaje']\n",
    "ham=entrenamiento[entrenamiento['etiqueta'] =='ham']['mensaje']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un nuevo objeto `CountVectorizer`para el conteo de palabras. Lo usaremos en el conteo de los mensajes SPAM, así como los de HAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "conts=CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es una variable auxiliar que contiene la totalidad de palabras dentro del diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_palabras_aux = diccionario['palabra']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para SPAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con la totalidad de palabras, realizaremos el conteo de las palabras dentro de los mensajes **spam**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = conts.fit_transform(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La extracción de características nos devolverá la lista de palabras encontradas dentro de estos mensajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pspam=conts.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la matriz de frecuencia de palabras para los mensajes **spam** y lo almacenamos en un `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_spam=pd.DataFrame(data=X.toarray(),columns=pspam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos una iteración de las palabras dentro del diccionario (`lista_palabras_aux`) y, en caso exista la palabra en la matriz de **spam**, obtendremos la sumatoria de la columna. En caso contrario, diremos que la palabra tiene **cero ocurrencias**. Esta lista tendrá el mismo tamaño de la lista de palabras del **diccionario general**, para nuestro ejemplo, 3000 palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_spam_actualizado = np.ndarray(dtype=np.int64, shape=(1,len(diccionario)))\n",
    "for i, palabra in enumerate(lista_palabras_aux):\n",
    "    if palabra in mf_spam:\n",
    "        mf_spam_actualizado.itemset(i, sum(mf_spam[palabra]))\n",
    "    else:\n",
    "        mf_spam_actualizado.itemset(i, 0)\n",
    "mf_spam_actualizado = np.ravel(mf_spam_actualizado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un DataFrame, al igual que con el diccionario general, para tener mejor visibilidad de las palabras con su ocurrencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_spam_diccionario = pd.DataFrame({'palabra':np.array(lista_palabras_aux), 'ocurrencia':mf_spam_actualizado})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palabra</th>\n",
       "      <th>ocurrencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>number</td>\n",
       "      <td>2483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>call</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ur</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>just</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>free</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ok</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ltgt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>know</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>day</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ll</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>got</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>good</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>like</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>come</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>time</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>send</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>love</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>want</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>text</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>going</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lor</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>today</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sorry</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>txt</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stop</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>home</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>need</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>don</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>back</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>dont</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>academic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>bp</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>customers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>galileo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>brah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>frog</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>brain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>hockey</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2978</th>\n",
       "      <td>fromm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2979</th>\n",
       "      <td>front</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>curry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2981</th>\n",
       "      <td>britney</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>british</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2983</th>\n",
       "      <td>challenge</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>count</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985</th>\n",
       "      <td>art</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>held</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>bulbs</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>cornwall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>boat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>geeeee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>aeroplane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>dealing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>freedom</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>aeronautics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>bloodsend</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>fails</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>bloodblood</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>advise</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>agree</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          palabra  ocurrencia\n",
       "0          number        2483\n",
       "1            call         283\n",
       "2              ur         110\n",
       "3            just          62\n",
       "4            free         178\n",
       "5              ok           3\n",
       "6            ltgt           0\n",
       "7            know          17\n",
       "8             day          20\n",
       "9              ll           3\n",
       "10            got           5\n",
       "11           good           8\n",
       "12           like          11\n",
       "13           come           1\n",
       "14           time          16\n",
       "15           send          50\n",
       "16           love           5\n",
       "17           want          21\n",
       "18           text          96\n",
       "19          going           2\n",
       "20            lor           0\n",
       "21          today          13\n",
       "22          sorry           3\n",
       "23            txt         111\n",
       "24           stop          92\n",
       "25           home           2\n",
       "26           need           9\n",
       "27            don           9\n",
       "28           back          17\n",
       "29           dont           9\n",
       "...           ...         ...\n",
       "2970     academic           0\n",
       "2971           bp           2\n",
       "2972    customers           2\n",
       "2973      galileo           0\n",
       "2974         brah           0\n",
       "2975         frog           2\n",
       "2976        brain           0\n",
       "2977       hockey           1\n",
       "2978        fromm           2\n",
       "2979        front           0\n",
       "2980        curry           0\n",
       "2981      britney           2\n",
       "2982      british           2\n",
       "2983    challenge           0\n",
       "2984        count           0\n",
       "2985          art           0\n",
       "2986         held           1\n",
       "2987        bulbs           2\n",
       "2988     cornwall           1\n",
       "2989         boat           0\n",
       "2990       geeeee           0\n",
       "2991    aeroplane           0\n",
       "2992      dealing           0\n",
       "2993      freedom           0\n",
       "2994  aeronautics           0\n",
       "2995    bloodsend           0\n",
       "2996        fails           0\n",
       "2997   bloodblood           0\n",
       "2998       advise           2\n",
       "2999        agree           0\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf_spam_diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTA: Realizaremos el mismo proceso para los mensajes `HAM`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts.fit(ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pham=conts.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pham=conts.transform(ham).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_ham=pd.DataFrame(data=arr_pham,columns=pham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_ham_actualizado = np.ndarray(dtype=np.int64, shape=(1,len(diccionario)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, palabra in enumerate(lista_palabras_aux):\n",
    "    if palabra in mf_ham:\n",
    "        mf_ham_actualizado.itemset(i, sum(mf_ham[palabra]))\n",
    "    else:\n",
    "        mf_ham_actualizado.itemset(i, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_ham_actualizado = np.ravel(mf_ham_actualizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_ham_diccionario = pd.DataFrame({'palabra':np.array(lista_palabras_aux), 'ocurrencia':mf_ham_actualizado})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2483,  283,  110, ...,    0,    2,    0], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diccionario[diccionario['palabra']=='number']\n",
    "mf_spam_actualizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palabra</th>\n",
       "      <th>ocurrencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>number</td>\n",
       "      <td>987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>call</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ur</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>just</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>free</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ok</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ltgt</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>know</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>day</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ll</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>got</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>good</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>like</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>come</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>time</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>send</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>love</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>want</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>text</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>going</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lor</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>today</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sorry</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>txt</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stop</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>home</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>need</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>don</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>back</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>dont</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>academic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>bp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>customers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>galileo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>brah</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>frog</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>brain</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>hockey</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2978</th>\n",
       "      <td>fromm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2979</th>\n",
       "      <td>front</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>curry</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2981</th>\n",
       "      <td>britney</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>british</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2983</th>\n",
       "      <td>challenge</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>count</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985</th>\n",
       "      <td>art</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>held</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>bulbs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>cornwall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>boat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>geeeee</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>aeroplane</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>dealing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>freedom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>aeronautics</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>bloodsend</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>fails</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>bloodblood</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>advise</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>agree</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          palabra  ocurrencia\n",
       "0          number         987\n",
       "1            call         168\n",
       "2              ur         180\n",
       "3            just         214\n",
       "4            free          45\n",
       "5              ok         201\n",
       "6            ltgt         193\n",
       "7            know         171\n",
       "8             day         168\n",
       "9              ll         180\n",
       "10            got         177\n",
       "11           good         174\n",
       "12           like         169\n",
       "13           come         174\n",
       "14           time         146\n",
       "15           send         101\n",
       "16           love         146\n",
       "17           want         126\n",
       "18           text          50\n",
       "19          going         134\n",
       "20            lor         126\n",
       "21          today         112\n",
       "22          sorry         121\n",
       "23            txt          13\n",
       "24           stop          30\n",
       "25           home         119\n",
       "26           need         106\n",
       "27            don         105\n",
       "28           back          94\n",
       "29           dont         101\n",
       "...           ...         ...\n",
       "2970     academic           2\n",
       "2971           bp           0\n",
       "2972    customers           0\n",
       "2973      galileo           2\n",
       "2974         brah           2\n",
       "2975         frog           0\n",
       "2976        brain           2\n",
       "2977       hockey           1\n",
       "2978        fromm           0\n",
       "2979        front           2\n",
       "2980        curry           2\n",
       "2981      britney           0\n",
       "2982      british           0\n",
       "2983    challenge           2\n",
       "2984        count           2\n",
       "2985          art           2\n",
       "2986         held           1\n",
       "2987        bulbs           0\n",
       "2988     cornwall           1\n",
       "2989         boat           2\n",
       "2990       geeeee           2\n",
       "2991    aeroplane           2\n",
       "2992      dealing           2\n",
       "2993      freedom           2\n",
       "2994  aeronautics           2\n",
       "2995    bloodsend           2\n",
       "2996        fails           2\n",
       "2997   bloodblood           2\n",
       "2998       advise           0\n",
       "2999        agree           2\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf_ham_diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de la técnica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta el momento hemos realizado los procesos para obtener nuestra data: palabras y su ocurrencia. Ahora realizaremos el algoritmo para obtener las probabilidades que nos servirán para determinar si un mensaje dado es **spam** o **ham**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0.001\n",
    "l=len(diccionario)\n",
    "p=sum(mf_ham_actualizado)\n",
    "probh = np.ndarray(dtype=np.float, shape=(1,l))\n",
    "for i in range(0,len(mf_ham_actualizado)):\n",
    "    probh.itemset(i,(mf_ham_actualizado[i]+a)/(p+(a*l)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=sum(mf_spam_actualizado)\n",
    "probs = np.ndarray(dtype=np.float, shape=(1,l))    \n",
    "for i in range(0,len(mf_spam_actualizado)):\n",
    "    probs.itemset(i,(mf_spam_actualizado[i]+a)/(p+(a*l)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=np.ravel(probs)\n",
    "probh=np.ravel(probh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_palabras=pd.DataFrame({'Palabras':lista_palabras_aux,'Probabilidad_Spam':probs,'Probabilidad_Ham':probh})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_h=len(mf_ham.index)/(len(mf_ham.index)+len(mf_spam.index))\n",
    "prob_s=len(mf_spam.index)/(len(mf_ham.index)+len(mf_spam.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq(str): \n",
    "    str = str.split()          \n",
    "    str2 = [] \n",
    "    for i in str: \n",
    "        if i not in str2:\n",
    "            str2.append(i)  \n",
    "    for i in range(0, len(str2)):\n",
    "        str2[i]=(str2[i],str.count(str2[i]))\n",
    "    return np.array(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_diccionario(str):\n",
    "    freq_total = freq(str)\n",
    "    freq_coincidentes = []\n",
    "    arr_palabras_diccionario = np.array(diccionario[\"palabra\"])\n",
    "    for palabra, frecuencia in freq_total:\n",
    "        if(palabra in arr_palabras_diccionario): # Palabra pertenece al diccionario\n",
    "            freq_coincidentes.append((palabra, frecuencia))\n",
    "    return freq_coincidentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq(\"you should invent your own phrases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_diccionario(\"ga\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_diccionario(msj_test[1447])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNB(mensaje):\n",
    "    nuevo_dic = freq_diccionario(mensaje) \n",
    "    probs= [0,0]\n",
    "    for clase in range(0,2):\n",
    "        probs[clase] = 0\n",
    "        for idxd in range (0,len(nuevo_dic)):\n",
    "            if clase == 0:\n",
    "                pdp=prob_palabras[prob_palabras.Palabras==nuevo_dic[idxd][0]].Probabilidad_Spam.item()\n",
    "            if clase == 1:\n",
    "                pdp=prob_palabras[prob_palabras.Palabras==nuevo_dic[idxd][0]].Probabilidad_Ham.item() \n",
    "            power = np.log(1+ int(nuevo_dic[idxd][1])) \n",
    "            probs[clase]+=power*np.log(pdp)\n",
    "        if clase == 0:\n",
    "            pdc = prob_s\n",
    "        if clase == 1:\n",
    "            pdc = prob_h\n",
    "        probs[clase] +=  np.log(pdc)\n",
    "    if probs[0] > probs[1]:\n",
    "        soh='spam'\n",
    "    if probs[0] < probs[1]:\n",
    "        soh='ham'\n",
    "    return soh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "equivocados_s = []\n",
    "equivocados_h = []\n",
    "for i in msj_test.keys():\n",
    "    if(MNB(msj_test[i])==etiqueta_test[i]):\n",
    "        count+=1\n",
    "    else:\n",
    "        if(etiqueta_test[i] == 'ham'):\n",
    "            equivocados_h.append(i)\n",
    "        else:\n",
    "            equivocados_s.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msj_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count/len(msj_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza las conversiones hechas al dataset sobre un mensaje dado\n",
    "def convertir_string_clasificador(mensaje):\n",
    "    # PROCESO DE CONVERSIÓN\n",
    "    df1 = pd.DataFrame({\"mensaje\":[mensaje]})\n",
    "    df1 = df1.apply(lambda x: x.astype(str).str.lower()) # To lower case\n",
    "    df1 = df1['mensaje'].str.replace('http\\S+|www.\\S+', 'httpaddr', case=False) # Reemplazamos urls\n",
    "    df1 = df1.replace('\\d+', ' number ', regex=True) # Reemplazamos los números\n",
    "    df1 = df1.replace('[,.;@#?!&$]+', '', regex=True) # Eliminamos los símbolos\n",
    "    return df1.to_string(index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNB_PROB(mensaje):\n",
    "    convertir_string_clasificador(mensaje)\n",
    "    nuevo_dic = freq_diccionario(mensaje) \n",
    "    probs= [0,0]\n",
    "    for clase in range(0,2):\n",
    "        probs[clase] = 0\n",
    "        for idxd in range (0,len(nuevo_dic)):\n",
    "            if clase == 0:\n",
    "                pdp=prob_palabras[prob_palabras.Palabras==nuevo_dic[idxd][0]].Probabilidad_Spam.item()\n",
    "            if clase == 1:\n",
    "                pdp=prob_palabras[prob_palabras.Palabras==nuevo_dic[idxd][0]].Probabilidad_Ham.item() \n",
    "            power = np.log(1+ int(nuevo_dic[idxd][1])) \n",
    "            probs[clase]+=power*np.log(pdp)\n",
    "        if clase == 0:\n",
    "            pdc = prob_s\n",
    "        if clase == 1:\n",
    "            pdc = prob_h\n",
    "        probs[clase] +=  np.log(pdc)\n",
    "    prob_posi = [0, 0]\n",
    "    prob_posi[0] = -1/probs[0]\n",
    "    prob_posi[1] = -1/probs[1]\n",
    "    total_posi = prob_posi[0] + prob_posi[1]\n",
    "    return [prob_posi[0]/total_posi, prob_posi[1]/total_posi]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
